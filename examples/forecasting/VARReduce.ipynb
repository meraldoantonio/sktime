{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate the functionality of the `VARReduce` model from sktime. We will compare its performance with the traditional `VAR` model to highlight their similar behavior under default settings. Additionally, we will showcase how `VARReduce` can be leveraged for regularization in scenarios involving large time series datasets, offering enhanced model performance and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Autoregression (VAR) is a powerful statistical tool used to capture the linear interdependencies among multiple time series. Despite its complex applications, the core mechanism of fitting a VAR model is fundamentally an extension of linear regression. This article explores how we can reframe the VAR fitting process into a more flexible framework by leveraging different types of regressors, including those that provide regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reframing VAR Model Fitting: From Linear Regression to Regularized Approaches\n",
    "\n",
    "Vector Autoregression (VAR) is a powerful statistical tool used to capture the linear interdependencies among multiple time series. Despite its complex applications, the core mechanism of fitting a VAR model is fundamentally an extension of linear regression. This article explores how we can reframe the VAR fitting process into a more flexible framework by leveraging different types of regressors, including those that provide regularization.\n",
    "\n",
    "#### Understanding Linear Regression\n",
    "\n",
    "Linear regression models the relationship between a dependent variable \\( y \\) and one or more independent variables \\( X \\). The model is expressed as:\n",
    "\n",
    "\\[ y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\cdots + \\beta_p x_{pt} + \\epsilon_t \\]\n",
    "\n",
    "Here:\n",
    "- \\( y_t \\) is the dependent variable at time \\( t \\).\n",
    "- \\( x_{it} \\) are the independent variables at time \\( t \\).\n",
    "- \\( \\beta_0, \\beta_1, \\ldots, \\beta_p \\) are the coefficients.\n",
    "- \\( \\epsilon_t \\) is the error term.\n",
    "\n",
    "The goal is to estimate the coefficients \\( \\beta \\) that minimize the sum of squared errors.\n",
    "\n",
    "#### The VAR Model\n",
    "\n",
    "A VAR model generalizes this concept to multiple time series. For \\( k \\) time series variables \\( y_{1,t}, y_{2,t}, \\ldots, y_{k,t} \\), a VAR model of order \\( p \\) (VAR(p)) is:\n",
    "\n",
    "\\[ \n",
    "\\begin{aligned}\n",
    "y_{1,t} &= c_1 + \\sum_{j=1}^{k} \\sum_{l=1}^{p} \\beta_{1j,l} y_{j,t-l} + \\epsilon_{1,t} \\\\\n",
    "y_{2,t} &= c_2 + \\sum_{j=1}^{k} \\sum_{l=1}^{p} \\beta_{2j,l} y_{j,t-l} + \\epsilon_{2,t} \\\\\n",
    "&\\vdots \\\\\n",
    "y_{k,t} &= c_k + \\sum_{j=1}^{k} \\sum_{l=1}^{p} \\beta_{kj,l} y_{j,t-l} + \\epsilon_{k,t} \n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Here:\n",
    "- \\( y_{i,t} \\) is the value of the \\( i \\)-th time series at time \\( t \\).\n",
    "- \\( c_i \\) are the intercepts.\n",
    "- \\( \\beta_{ij,l} \\) are the coefficients for the \\( j \\)-th variable's \\( l \\)-lag effect on the \\( i \\)-th variable.\n",
    "- \\( \\epsilon_{i,t} \\) are the error terms.\n",
    "\n",
    "### Reframing VAR Fitting: Tabularization Followed by Regression\n",
    "\n",
    "To simplify the VAR fitting process, we can transform the time series data into a tabular (matrix) format and then apply regression techniques.\n",
    "\n",
    "#### Step 1: Tabularization (Matrix Formulation)\n",
    "\n",
    "- **Time Series to Tabular Data**: Convert each time series into a matrix where each row represents a time point and the columns represent the lagged values of all time series variables.\n",
    "- **Example**: For a VAR(2) model with two time series \\( y_1 \\) and \\( y_2 \\), the matrix might look like this:\n",
    "\n",
    "  \\[\n",
    "  \\begin{pmatrix}\n",
    "  y_{1,t-1} & y_{2,t-1} & y_{1,t-2} & y_{2,t-2} \\\\\n",
    "  y_{1,t} & y_{2,t} & y_{1,t-1} & y_{2,t-1} \\\\\n",
    "  y_{1,t+1} & y_{2,t+1} & y_{1,t} & y_{2,t} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots\n",
    "  \\end{pmatrix}\n",
    "  \\]\n",
    "\n",
    "- **Response Variables**: Create response vectors for each time series variable representing the value at the current time point.\n",
    "- **Example**: For \\( y_1 \\), the response vector might be:\n",
    "\n",
    "  \\[\n",
    "  \\begin{pmatrix}\n",
    "  y_{1,t} \\\\\n",
    "  y_{1,t+1} \\\\\n",
    "  y_{1,t+2} \\\\\n",
    "  \\vdots\n",
    "  \\end{pmatrix}\n",
    "  \\]\n",
    "\n",
    "#### Step 2: Regression\n",
    "\n",
    "Once we have our data in a tabular format, we can use various regression techniques to estimate the coefficients.\n",
    "\n",
    "### Alternative Regressors for Regularized VAR\n",
    "\n",
    "1. **Ridge Regression (L2 Regularization)**:\n",
    "   - Adds a penalty proportional to the sum of the squares of the coefficients.\n",
    "   - Helps prevent overfitting by shrinking the coefficients.\n",
    "\n",
    "     \\[\n",
    "     \\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "     \\]\n",
    "\n",
    "2. **Lasso Regression (L1 Regularization)**:\n",
    "   - Adds a penalty proportional to the sum of the absolute values of the coefficients.\n",
    "   - Can drive some coefficients to zero, effectively performing variable selection.\n",
    "\n",
    "     \\[\n",
    "     \\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "     \\]\n",
    "\n",
    "3. **Elastic Net Regression**:\n",
    "   - Combines L1 and L2 regularization.\n",
    "   - Balances between ridge and lasso by tuning a mixing parameter.\n",
    "\n",
    "     \\[\n",
    "     \\min_{\\beta} \\sum_{i=1}^{n} (y_i - X_i \\beta)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n",
    "     \\]\n",
    "\n",
    "### Steps to Implement Regularized VAR\n",
    "\n",
    "1. **Tabularize the Data**:\n",
    "   - Construct the lagged matrix \\( X \\) and response vectors \\( Y_1, Y_2, \\ldots, Y_k \\) for each time series.\n",
    "\n",
    "2. **Choose a Regularized Regressor**:\n",
    "   - Use Ridge, Lasso, or Elastic Net regression to fit each response vector on the lagged matrix.\n",
    "\n",
    "3. **Estimate Coefficients**:\n",
    "   - Solve the regression problem for each response vector to get the coefficients.\n",
    "\n",
    "4. **Construct the VAR Model**:\n",
    "   - Use the estimated coefficients to form the VAR model, which now includes regularization.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Hereâ€™s a brief example using Python and scikit-learn to demonstrate how you might implement regularized VAR:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is the lagged matrix and Y is the response vector for a single time series\n",
    "\n",
    "# Example data (replace with actual lagged data)\n",
    "X = np.random.rand(100, 4)  # 100 time points, 4 lagged variables\n",
    "Y = np.random.rand(100)     # 100 time points\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, Y)\n",
    "ridge_coefficients = ridge.coef_\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, Y)\n",
    "lasso_coefficients = lasso.coef_\n",
    "\n",
    "# Elastic Net Regression\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic_net.fit(X, Y)\n",
    "elastic_net_coefficients = elastic_net.coef_\n",
    "\n",
    "print(\"Ridge Coefficients:\", ridge_coefficients)\n",
    "print(\"Lasso Coefficients:\", lasso_coefficients)\n",
    "print(\"Elastic Net Coefficients:\", elastic_net_coefficients)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By reframing the problem of fitting a VAR model as tabularization followed by regression, we open up a range of possibilities for using different types of regressors. This approach not only simplifies the implementation but also allows for the incorporation of regularization techniques like Ridge, Lasso, and Elastic Net. These regularized approaches can enhance the flexibility and robustness of the VAR model, making it more capable of handling complex datasets and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "from sktime.forecasting.var_reduce import VARReduce\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.forecasting.compose import ForecastingPipeline\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the comparison between `VAR` and `VARReduce`, we first generate a small synthetic multivariate time series dataset. \n",
    "\n",
    "We create three time series, each with 100 observations, and store them in a DataFrame. \n",
    "\n",
    "We then split the data into training and testing sets, reserving the last 5 observations for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_obs = 100\n",
    "time_series_1 = np.random.randn(n_obs) + 1000\n",
    "time_series_2 = np.random.randn(n_obs) + 100\n",
    "time_series_3 = np.random.randn(n_obs) + 10\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"ts1\": time_series_1,\n",
    "        \"ts2\": time_series_2,\n",
    "        \"ts3\": time_series_3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "df_train, df_test = temporal_train_test_split(df, test_size=5)\n",
    "\n",
    "# Define the forecasting horizon\n",
    "fh = ForecastingHorizon(df_test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VARReduce` model applies Vector Autoregression (VAR) with optional regularization. \n",
    "It operates by converting multivariate time series data into a tabular format suitable for regression models. By default, `VARReduce` uses `scikit-learn`'s `LinearRegression` to perform the fitting and behaves like a traditional VAR model. However, users can specify other `scikit-learn` compatible regressors to introduce regularization, enhancing performance for large datasets or when multicollinearity is present.\n",
    "\n",
    "Here's how the `VARReduce` model works step-by-step:\n",
    "\n",
    "1. Data Preparation: The time series data is transformed into a format where each row contains lagged values of the series, turning it into a tabular dataset.\n",
    "2. Model Fitting: The model uses the specified regressor to fit the lagged values as predictors and the current values as the response variable.\n",
    "3. Forecasting: The fitted model is used to forecast future values based on the most recent observations.\n",
    "\n",
    "To demonstrate the intermediate DataFrame generated during the data preparation step, we'll take a peep at the internal `prepare_var_data` method from the `VARReduce` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lags\n",
    "LAGS = 2\n",
    "\n",
    "# Instantiate and fit the custom VARReduce model\n",
    "varreduce_model = VARReduce(lags=LAGS, regressor = Ridge(alpha = 100))  # no regressor passed, by default, LinearRegressor() will be used\n",
    "varreduce_model.fit(df_train)\n",
    "\n",
    "# Fit the VAR model comparison\n",
    "var_model = VAR(endog=df_train)\n",
    "\n",
    "# Fit the VAR model using statsmodels for comparison\n",
    "var_model = VAR(df_train)\n",
    "results =var_model.fit(LAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lags\n",
    "LAGS = 2\n",
    "\n",
    "# Instantiate and fit the custom VARReduce model\n",
    "varreduce_model = VARReduce(lags=LAGS, regressor = Ridge(alpha = 0.2))  # no regressor passed, by default, LinearRegressor() will be used\n",
    "varreduce_model.fit(df_train)\n",
    "\n",
    "# Fit the VAR model comparison\n",
    "var_model = VAR(endog=df_train)\n",
    "\n",
    "# Fit the VAR model using statsmodels for comparison\n",
    "var_model = VAR(df_train)\n",
    "results =var_model.fit(LAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We observe that internally, `VARReduce` transforms the provided training data into a tabular format suitable for regression. Specifically:\n",
    "\n",
    "1. **Predictors (X)**: The predictors consist of lagged values of the time series, with the number of lags specified by the `lags` parameter. In the displayed DataFrame, each column represents a lagged value for one of the time series (e.g., `ts1_lag1`, `ts2_lag2`).\n",
    "\n",
    "2. **Target Variables (y)**: The target variables are the current, unlagged values of the time series, shown in columns such as `ts1_lag0`, `ts2_lag0`, and `ts3_lag0`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts1_lag1</th>\n",
       "      <th>ts2_lag1</th>\n",
       "      <th>ts3_lag1</th>\n",
       "      <th>ts1_lag2</th>\n",
       "      <th>ts2_lag2</th>\n",
       "      <th>ts3_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999.861736</td>\n",
       "      <td>99.579355</td>\n",
       "      <td>10.560785</td>\n",
       "      <td>1000.496714</td>\n",
       "      <td>98.584629</td>\n",
       "      <td>10.357787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.647689</td>\n",
       "      <td>99.657285</td>\n",
       "      <td>11.083051</td>\n",
       "      <td>999.861736</td>\n",
       "      <td>99.579355</td>\n",
       "      <td>10.560785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001.523030</td>\n",
       "      <td>99.197723</td>\n",
       "      <td>11.053802</td>\n",
       "      <td>1000.647689</td>\n",
       "      <td>99.657285</td>\n",
       "      <td>11.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>999.765847</td>\n",
       "      <td>99.838714</td>\n",
       "      <td>8.622331</td>\n",
       "      <td>1001.523030</td>\n",
       "      <td>99.197723</td>\n",
       "      <td>11.053802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>999.765863</td>\n",
       "      <td>100.404051</td>\n",
       "      <td>9.062175</td>\n",
       "      <td>999.765847</td>\n",
       "      <td>99.838714</td>\n",
       "      <td>8.622331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ts1_lag1    ts2_lag1   ts3_lag1     ts1_lag2   ts2_lag2   ts3_lag2\n",
       "2   999.861736   99.579355  10.560785  1000.496714  98.584629  10.357787\n",
       "3  1000.647689   99.657285  11.083051   999.861736  99.579355  10.560785\n",
       "4  1001.523030   99.197723  11.053802  1000.647689  99.657285  11.083051\n",
       "5   999.765847   99.838714   8.622331  1001.523030  99.197723  11.053802\n",
       "6   999.765863  100.404051   9.062175   999.765847  99.838714   8.622331"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = varreduce_model.prepare_var_data(df_train, return_as_ndarray=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts1_lag0</th>\n",
       "      <th>ts2_lag0</th>\n",
       "      <th>ts3_lag0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000.647689</td>\n",
       "      <td>99.657285</td>\n",
       "      <td>11.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001.523030</td>\n",
       "      <td>99.197723</td>\n",
       "      <td>11.053802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>999.765847</td>\n",
       "      <td>99.838714</td>\n",
       "      <td>8.622331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>999.765863</td>\n",
       "      <td>100.404051</td>\n",
       "      <td>9.062175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001.579213</td>\n",
       "      <td>101.886186</td>\n",
       "      <td>10.515035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ts1_lag0    ts2_lag0   ts3_lag0\n",
       "2  1000.647689   99.657285  11.083051\n",
       "3  1001.523030   99.197723  11.053802\n",
       "4   999.765847   99.838714   8.622331\n",
       "5   999.765863  100.404051   9.062175\n",
       "6  1001.579213  101.886186  10.515035"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is prepared in this tabular format, `VARReduce` trains a regressor on it. By default, `LinearRegression` from scikit-learn is used as the regressor. Under these default conditions, the `VARReduce` model behaves just like a traditional VAR model. This means that the fitted coefficients and the resulting forecasts will be identical to those of a VAR model, as both are essentially performing linear regression on the same lagged data.\n",
    "\n",
    "This equivalence is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients from VARReduce model:\n",
      "[[[-0.04361311 -0.1432403  -0.0286208 ]\n",
      "  [-0.02681481 -0.12353761 -0.09681436]\n",
      "  [-0.02880361 -0.12741472 -0.11312612]]\n",
      "\n",
      " [[-0.04871809 -0.05343262 -0.01240621]\n",
      "  [ 0.11886914 -0.05718469 -0.131875  ]\n",
      "  [ 0.09278917  0.09723644 -0.03899424]]]\n",
      "\n",
      "Coefficients from statsmodels VAR:\n",
      "[[[-0.04379616 -0.14362626 -0.02868124]\n",
      "  [-0.02691002 -0.12388731 -0.09702357]\n",
      "  [-0.02884349 -0.12771343 -0.11331339]]\n",
      "\n",
      " [[-0.0488787  -0.0536462  -0.01244942]\n",
      "  [ 0.11919954 -0.05735081 -0.13221751]\n",
      "  [ 0.0930699   0.09744015 -0.0391535 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Compare coefficients\n",
    "print(\"Coefficients from VARReduce model:\")\n",
    "print(varreduce_model.coefficients_)\n",
    "print(\"\\nCoefficients from statsmodels VAR:\")\n",
    "print(results.coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the two models have essentially identical fitted parameters, it is unsurprising that their forecasts are also essentially identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating forecasts from both models\n",
    "df_pred_varreduce = varreduce_model.predict(fh=fh)\n",
    "df_pred_statsmodels = results.forecast(df_train.values[-LAGS:], steps=len(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts1</th>\n",
       "      <th>ts2</th>\n",
       "      <th>ts3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999.953792</td>\n",
       "      <td>99.960219</td>\n",
       "      <td>9.816463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999.918117</td>\n",
       "      <td>100.005744</td>\n",
       "      <td>10.045984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>999.907816</td>\n",
       "      <td>100.103436</td>\n",
       "      <td>10.047713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>999.890680</td>\n",
       "      <td>100.054364</td>\n",
       "      <td>10.027533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>999.894295</td>\n",
       "      <td>100.055801</td>\n",
       "      <td>10.045038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts1         ts2        ts3\n",
       "1  999.953792   99.960219   9.816463\n",
       "2  999.918117  100.005744  10.045984\n",
       "3  999.907816  100.103436  10.047713\n",
       "4  999.890680  100.054364  10.027533\n",
       "5  999.894295  100.055801  10.045038"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_varreduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[999.95405968,  99.95999686,   9.81591952],\n",
       "       [999.9181998 , 100.00566178,  10.04602297],\n",
       "       [999.9078646 , 100.10367569,  10.04778239],\n",
       "       [999.89062779, 100.05432332,  10.0274662 ],\n",
       "       [999.89427888, 100.05578669,  10.04508813]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the predictions from VARReduce and sktime VAR close?  False\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions using numpy's allclose function\n",
    "epsilon = 1e-8\n",
    "predictions_are_close = np.allclose(df_pred_varreduce, df_pred_statsmodels, atol=epsilon)\n",
    "\n",
    "print(\"Are the predictions from VARReduce and sktime VAR close? \", predictions_are_close)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Real World Macreconomic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of `VARReduce` becomes more evident when dealing with real-world datasets containing numerous time series. An example is the `macrodata` dataset, which contains 14 variables. This dataset is loaded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realgdp</th>\n",
       "      <th>realcons</th>\n",
       "      <th>realinv</th>\n",
       "      <th>realgovt</th>\n",
       "      <th>realdpi</th>\n",
       "      <th>cpi</th>\n",
       "      <th>m1</th>\n",
       "      <th>tbilrate</th>\n",
       "      <th>unemp</th>\n",
       "      <th>pop</th>\n",
       "      <th>infl</th>\n",
       "      <th>realint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959Q1</th>\n",
       "      <td>2710.349</td>\n",
       "      <td>1707.4</td>\n",
       "      <td>286.898</td>\n",
       "      <td>470.045</td>\n",
       "      <td>1886.9</td>\n",
       "      <td>28.98</td>\n",
       "      <td>139.7</td>\n",
       "      <td>2.82</td>\n",
       "      <td>5.8</td>\n",
       "      <td>177.146</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959Q2</th>\n",
       "      <td>2778.801</td>\n",
       "      <td>1733.7</td>\n",
       "      <td>310.859</td>\n",
       "      <td>481.301</td>\n",
       "      <td>1919.7</td>\n",
       "      <td>29.15</td>\n",
       "      <td>141.7</td>\n",
       "      <td>3.08</td>\n",
       "      <td>5.1</td>\n",
       "      <td>177.830</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959Q3</th>\n",
       "      <td>2775.488</td>\n",
       "      <td>1751.8</td>\n",
       "      <td>289.226</td>\n",
       "      <td>491.260</td>\n",
       "      <td>1916.4</td>\n",
       "      <td>29.35</td>\n",
       "      <td>140.5</td>\n",
       "      <td>3.82</td>\n",
       "      <td>5.3</td>\n",
       "      <td>178.657</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959Q4</th>\n",
       "      <td>2785.204</td>\n",
       "      <td>1753.7</td>\n",
       "      <td>299.356</td>\n",
       "      <td>484.052</td>\n",
       "      <td>1931.3</td>\n",
       "      <td>29.37</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.6</td>\n",
       "      <td>179.386</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960Q1</th>\n",
       "      <td>2847.699</td>\n",
       "      <td>1770.5</td>\n",
       "      <td>331.722</td>\n",
       "      <td>462.199</td>\n",
       "      <td>1955.5</td>\n",
       "      <td>29.54</td>\n",
       "      <td>139.6</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.2</td>\n",
       "      <td>180.007</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         realgdp  realcons  realinv  realgovt  realdpi    cpi     m1  \\\n",
       "date                                                                   \n",
       "1959Q1  2710.349    1707.4  286.898   470.045   1886.9  28.98  139.7   \n",
       "1959Q2  2778.801    1733.7  310.859   481.301   1919.7  29.15  141.7   \n",
       "1959Q3  2775.488    1751.8  289.226   491.260   1916.4  29.35  140.5   \n",
       "1959Q4  2785.204    1753.7  299.356   484.052   1931.3  29.37  140.0   \n",
       "1960Q1  2847.699    1770.5  331.722   462.199   1955.5  29.54  139.6   \n",
       "\n",
       "        tbilrate  unemp      pop  infl  realint  \n",
       "date                                             \n",
       "1959Q1      2.82    5.8  177.146  0.00     0.00  \n",
       "1959Q2      3.08    5.1  177.830  2.34     0.74  \n",
       "1959Q3      3.82    5.3  178.657  2.74     1.09  \n",
       "1959Q4      4.33    5.6  179.386  0.27     4.06  \n",
       "1960Q1      3.50    5.2  180.007  2.31     1.19  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the macrodata dataset using statsmodels\n",
    "dataset = sm.datasets.macrodata.load_pandas()\n",
    "df = dataset.data\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['quarter'] = df['quarter'].astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "df['date'] = pd.to_datetime(df['year'].astype(str) + 'Q' + df['quarter'].astype(str))\n",
    "\n",
    "# Set the datetime column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Convert the datetime index to a period index with quarterly frequency\n",
    "df.index = df.index.to_period('Q')\n",
    "\n",
    "# Drop the original year and quarter columns if no longer needed\n",
    "df.drop(columns=['year', 'quarter'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "df_train, df_test = temporal_train_test_split(df, test_size=24)\n",
    "fh = ForecastingHorizon(df_test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a traditional VAR model on this dataset with a lag of 3 means we have to fit a substantial number of parameters. Specifically, for each variable, the model needs to estimate coefficients for the current and three previous values of all 14 variables, plus an intercept. This results in a total of \\( (14 \\times 14 \\times 3) + 14 = 602 \\) parameters. Given that the number of data points is relatively small, this high number of parameters makes the model prone to overfitting, where it captures noise in the training data rather than the underlying patterns.\n",
    "\n",
    "Overfitting leads to poor generalization to new, unseen data, resulting in inaccurate forecasts. To mitigate this, regularization techniques can be applied, which is where `VARReduce` shines. By selecting a regressor with built-in regularization properties, such as Ridge regression (L2 regularization) or Lasso regression (L1 regularization), we can introduce penalties on the size of the coefficients. This effectively controls the complexity of the model, preventing overfitting, enhancing stability, and improving the forecast accuracy.\n",
    "\n",
    "For instance, using Ridge regression within `VARReduce` helps to manage multicollinearity and shrink the less important coefficients towards zero, making the model more interpretable and reliable. This ability to integrate advanced regression techniques allows `VARReduce` to leverage the flexibility and robustness of scikit-learn regressors, providing a powerful tool for time series forecasting in complex, high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object VARReduce(lags=3, regressor=Ridge(alpha=10)), as the constructor either does not set or modifies parameter regressor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m TabularToSeriesAdaptor(StandardScaler())\n\u001b[1;32m      6\u001b[0m varreduce_model \u001b[38;5;241m=\u001b[39m VARReduce(lags\u001b[38;5;241m=\u001b[39mLAGS, regressor \u001b[38;5;241m=\u001b[39m Ridge(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m pipeline_varreduce \u001b[38;5;241m=\u001b[39m \u001b[43mForecastingPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforecaster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarreduce_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pipeline_varreduce\u001b[38;5;241m.\u001b[39mfit(df_train)\n",
      "File \u001b[0;32m~/Desktop/sktime_dev/sktime/sktime/forecasting/compose/_pipeline.py:405\u001b[0m, in \u001b[0;36mForecastingPipeline.__init__\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, steps):\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m steps\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_postproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    407\u001b[0m     tags_to_clone \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignores-exogeneous-X\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# does estimator ignore the exogeneous X?\u001b[39;00m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapability:pred_int\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# can the estimator produce prediction intervals?\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menforce_index_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# index type that needs to be enforced in X/y\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/sktime_dev/sktime/sktime/forecasting/compose/_pipeline.py:83\u001b[0m, in \u001b[0;36m_Pipeline._check_steps\u001b[0;34m(self, estimators, allow_postproc)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# if len(estimators) == 1:\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     msg = (\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#         f\"in {self_name}, found steps of length 1, \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#     warn(msg, obj=self)\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m estimator_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_estimator_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_ests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m names, estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mestimator_tuples)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# validate names\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sktime_dev/sktime/sktime/base/_meta.py:388\u001b[0m, in \u001b[0;36m_HeterogenousMetaEstimator._get_estimator_tuples\u001b[0;34m(self, estimators, clone_ests)\u001b[0m\n\u001b[1;32m    386\u001b[0m ests \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_estimator_list(estimators)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_ests:\n\u001b[0;32m--> 388\u001b[0m     ests \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ests]\n\u001b[1;32m    389\u001b[0m unique_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_estimator_names(estimators, make_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    390\u001b[0m est_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(unique_names, ests))\n",
      "File \u001b[0;32m~/Desktop/sktime_dev/sktime/sktime/base/_meta.py:388\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m ests \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_estimator_list(estimators)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_ests:\n\u001b[0;32m--> 388\u001b[0m     ests \u001b[38;5;241m=\u001b[39m [\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ests]\n\u001b[1;32m    389\u001b[0m unique_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_estimator_names(estimators, make_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    390\u001b[0m est_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(unique_names, ests))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/skbase/base/_base.py:160\u001b[0m, in \u001b[0;36mBaseObject.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Obtain a clone of the object with same hyper-parameters.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    A clone is a different object without shared references, in post-init state.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    If successful, equal in value to ``type(self)(**self.get_params(deep=False))``.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     self_clone \u001b[38;5;241m=\u001b[39m \u001b[43m_clone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_clone\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    162\u001b[0m         _check_clone(original\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, clone\u001b[38;5;241m=\u001b[39mself_clone)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/skbase/base/_base.py:1383\u001b[0m, in \u001b[0;36m_clone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     param2 \u001b[38;5;241m=\u001b[39m params_set[name]\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param2:\n\u001b[0;32m-> 1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, as the constructor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither does not set or modifies parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator, name)\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# This is an extension to the original sklearn implementation\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, BaseObject) \u001b[38;5;129;01mand\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mget_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclone_config\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object VARReduce(lags=3, regressor=Ridge(alpha=10)), as the constructor either does not set or modifies parameter regressor"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define lags\n",
    "LAGS = 3\n",
    "\n",
    "# Create the pipeline for VARReduce\n",
    "scaler = TabularToSeriesAdaptor(StandardScaler())\n",
    "varreduce_model = VARReduce(lags=LAGS, regressor = Ridge(alpha = 10))\n",
    "pipeline_varreduce = ForecastingPipeline(steps=[(\"scaler\", scaler), (\"forecaster\", varreduce_model)])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline_varreduce.fit(df_train)\n",
    "df_pred_varreduce = pipeline_varreduce.predict(fh=fh)\n",
    "\n",
    "# Create the pipeline for VAR using sktime\n",
    "pipeline_var = ForecastingPipeline(steps=[(\"scaler\", scaler), (\"forecaster\", VAR(maxlags=LAGS))])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline_var.fit(df_train)\n",
    "df_pred_var = pipeline_var.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for VARReduce\n",
    "mape_varreduce = mean_absolute_percentage_error(df_test, df_pred_varreduce)\n",
    "print(f\"MAPE for VARReduce model: {mape_varreduce:.2f}\")\n",
    "\n",
    "# Calculate performance metrics for statsmodels VAR\n",
    "mape_var = mean_absolute_percentage_error(df_test, df_pred_var)\n",
    "print(f\"MAPE for statsmodels VAR model: {mape_var:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
